<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Lab 1.2 - Tracking | CMPUT 428/615 Labs </title> <meta name="author" content="Computer Vision and Robotics Research Group "> <meta name="description" content="Lab resources for CMPUT428/615."> <meta name="keywords" content="cmput428, cmput615, ualberta, computing-science academic-website"> <link rel="stylesheet" href="/cmput428labs/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/cmput428labs/assets/img/favi.ico?824de67f88398f637ce988037dfe4447"> <link rel="stylesheet" href="/cmput428labs/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ualberta-vision-robotics.github.io/cmput428labs/tracking"> <script src="/cmput428labs/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/cmput428labs/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/cmput428labs/"> CMPUT 428/615 Labs </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/cmput428labs/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cmput428labs/policies">Policies </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Labs </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cmput428labs/opticalflow">Lab 1.1</a> <a class="dropdown-item " href="/cmput428labs/tracking">Lab 1.2</a> <a class="dropdown-item " href="/cmput428labs/2dgeometry">Lab 2.1</a> <a class="dropdown-item " href="/cmput428labs/3dgeometry">Lab 2.2</a> <a class="dropdown-item " href="/cmput428labs/structurefrommotion">Lab 3</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Lab 1.2 - Tracking</h1> <p class="post-description"></p> </header> <article> <div class="row justify-content-md-center"> <div class="col-sm-3 mt-4 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cmput428labs/assets/img/tracker-480.webp 480w,/cmput428labs/assets/img/tracker-800.webp 800w,/cmput428labs/assets/img/tracker-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/cmput428labs/assets/img/tracker.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="optical flow" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> To change: filler image of tracking a banana. </div> <h2 id="overview">Overview</h2> <p>In this lab you’ll implement the Lucas-Kanade tracking algorithm. The goal of this algorithm is to minimize the sum of squared error between two regions, the template \(T(\mathbf{x})\), and the warped image \(I(\mathbf{W}(\mathbf{x} ; \mathbf{p}))\):</p> <p>\begin{equation} \sum_{\mathbf{x}}[I(\mathbf{W}(\mathbf{x} ; \mathbf{p}))-T(\mathbf{x})]^2 \end{equation}</p> <p>\(\mathbf{p}\) is the vector of parameters that defines our warp. The most basic of which would be a simple 2-dof translation. Higher order warps such as an affine warp can also be applied.</p> <p>The user first defines the tracking region of interest, which becomes the first template. We aim to find the warped region in the next frame that best matches our template. When a suitable match is found, we set the warped region as the new template and repeat on the next frame.</p> <p><strong>Important: You can obtain 4 bonus marks for each section you finish and demo while present during the lab period we introduce the lab.</strong></p> <hr> <h2 id="1-2-dof-ssd-tracker-50">1. 2-Dof SSD Tracker (50)</h2> <p>Create a function <code class="language-plaintext highlighter-rouge">simple_tracker(img0, img1, roi, max_iterations, threshold)</code> takes a pair of sequenced images and a tracking region and returns the coordinates of the updated tracking region.</p> <h3 id="a-apply-your-tracker-to-a-pair-of-images">a) Apply your tracker to a pair of images.</h3> <ul> <li>Display the first image and define a bounding box to track using <code class="language-plaintext highlighter-rouge">cv2.selectROI()</code>. This defines the <code class="language-plaintext highlighter-rouge">roi</code> for your function.</li> <li>For the next frame, solve for \(\mathbf{u}=[u,v]^T\) on the tracked region like in optical flow. Your tracked region is the patch. <ul> <li>Use \(\mathbf{u}\) to update the tracked region. <ul> <li>Repeatedly solve for \(\mathbf{u}\) and update the tracking region until <code class="language-plaintext highlighter-rouge">norm($$u_k$$)/norm($$u_{k-1}$$) &lt; threshold</code> or <code class="language-plaintext highlighter-rouge">k &lt; max_iterations</code>, where <code class="language-plaintext highlighter-rouge">k</code> is the number of iterations (you can define your own end condition, this is just what we suggest).</li> </ul> </li> <li>Your function should return the new <code class="language-plaintext highlighter-rouge">roi</code>. Update and display the this bounding box as the new tracked region (see <code class="language-plaintext highlighter-rouge">cv2.rectangle</code> for plotting).</li> </ul> </li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Display the pair of images in your report.</li> </ul> <p><strong>Report Question 1a:</strong> Name one type of image processing that may improve the performance of your tracker.</p> <h3 id="b-create-a-live-implementation-of-your-tracker">b) Create a live implementation of your tracker.</h3> <p><strong>Report Question 1b:</strong> In what cases does the tracker perform well? In what cases does the tracker perform poorly?</p> <h3 id="c-apply-your-tracker-to-a-video-sequence-that-you-recorded">c) Apply your tracker to a video sequence that you recorded.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save your video with the overlaid tracking region for your submission. Ensure the video is less than 10 mb.</li> </ul> <p><strong>Report Question 1c:</strong> Name one advantage and one disadvantage of using higher order warps for our tracker.</p> <hr> <p>###<strong>Important</strong></p> <ul> <li> <p>Undergraduate students: choose <strong>one of the following three questions</strong> to complete for this lab to obtain the remaining 50 marks.</p> </li> <li> <p><strong><font color="DarkViolet">Grad students: complete question 2 and 3 OR complete question 4 to obtain the remaining 50 marks.</font></strong></p> </li> </ul> <hr> <h2 id="2-pyramidal-2-dof-ssd-tracker">2. Pyramidal 2-Dof SSD Tracker</h2> <p>Create a function <code class="language-plaintext highlighter-rouge">pyramidal_tracker(img0, img1, roi, levels=4, scale=2)</code> that performs 2-Dof SSD tracking with pyramidal guassian downsampling. You may call <code class="language-plaintext highlighter-rouge">simple_tracker()</code> within this function if you wish.</p> <ul> <li>The gaussian pyramid has <code class="language-plaintext highlighter-rouge">levels</code> layers, with each layer having a <code class="language-plaintext highlighter-rouge">scale</code> factor resolution of the layer above.</li> <li>See <code class="language-plaintext highlighter-rouge">cv2.pyrDown()</code> for gaussian downsampling.</li> <li>For each frame start with the top of the pyramid (coarsest) and solve for the new region. Propogate the result to the more detailed layer below and repeat for all layers. <ul> <li>Ensure the coordinates of the updated region match the scale factor of the layer below (i.e. multiply the translation by <code class="language-plaintext highlighter-rouge">scale</code>)</li> </ul> </li> <li>Update the tracked region with the bounding box of the bottom layer (the layer with no downsampling) and repeat for all frames.</li> </ul> <h3 id="apply-your-pyramidal-tracker-to-a-video-that-you-recorded">Apply your pyramidal tracker to a video that you recorded.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save this video to include with your submission. Ensure it is less than 10 mb.</li> </ul> <p><strong>Report Question 2:</strong> What are two benefits of implementing pyramidal downsampling to our tracker.</p> <hr> <h2 id="3-high-dof-tracker">3. High-Dof Tracker</h2> <p>Create a function <code class="language-plaintext highlighter-rouge">highdof_tracker(img0, img1, roi, max_iterations, threshold)</code> that warps the bounding box using 4 (x, y, rotation, scale) or 6 (affine) parameters \(\mathbf{p}\).</p> <ul> <li>See <a href="https://docs.opencv.org/4.x/da/d6e/tutorial_py_geometric_transformations.html" rel="external nofollow noopener" target="_blank">OpenCV documentation on transformations</a>.</li> <li>See <a href="https://www.ncorr.com/download/publications/bakerunify.pdf" rel="external nofollow noopener" target="_blank">Baker and Matthews Paper on Lukas-Kanade</a> </li> <li>See <a href="https://ugweb.cs.ualberta.ca/~vis/courses/CompVis/lectures24/lec05bRegTrack2.pdf" rel="external nofollow noopener" target="_blank">page 27 of these slides</a> </li> </ul> <h3 id="a-apply-your-tracker-to-a-video-with-object-motion">a) Apply your tracker to a video with object motion.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save the video to include in your submission. Ensure it is less than 10 mb.</li> </ul> <p><strong>Report Question 1a:</strong> lorem ipsum?</p> <h3 id="b-apply-your-tracker-to-a-video-with-camera-motion">b) Apply your tracker to a video with camera motion.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save the video to include in your submission. Ensure it is less than 10 mb.</li> </ul> <p><strong>Report Question 1a:</strong> lorem ipsum?</p> <hr> <h2 id="4-learning-based-tracking">4. Learning-Based Tracking</h2> <p>In traditional intensity-based tracking, we estimate the parameters of a transformation that warps a template (I_0) to match a region in the current image (I_t). Typically, this is approached by minimizing the intensity error between the template and the warped patch.</p> <p>However, instead of <strong>analytically computing</strong> the Jacobian (as you did in previous exercises), we now want to <strong>learn</strong> the mapping from image intensities to transformation parameters from a <strong>large set of synthetically generated training samples</strong>.</p> <p>You will implement <strong>one</strong> of the following methods (or <strong>both</strong>, for double credit):</p> <ol> <li> <p><strong>Hyperplane Approximation</strong><br> <em>Reference</em>: F. Jurie and M. Dhome, “Hyperplane approximation for template matching,” 2002.</p> </li> <li> <p><strong>Nearest Neighbor Approximation</strong><br> <em>Reference</em>: D. Travis, C. Perez, A. Shademan, and M. Jagersand, “Realtime Registration-Based Tracking via Approximate Nearest Neighbour Search,” 2013.</p> </li> </ol> <p>A quick overview of these learning-based approaches can be found in (<code class="language-plaintext highlighter-rouge">lec05bRegTrack2.pdf</code> slides 47-52).</p> <hr> <h3 id="tasks">Tasks:</h3> <h4 id="1-sampling-the-region-sample_region">1. Sampling the Region (<code class="language-plaintext highlighter-rouge">sample_region</code>)</h4> <ul> <li>Write a function <strong><code class="language-plaintext highlighter-rouge">sample_region</code></strong> that takes as input the four corner coordinates of a region in an image and returns a rectangular patch corresponding to that region.</li> <li>You can implement this by estimating a <strong>DLT (Direct Linear Transform)</strong> that warps the four-corner region into a fixed rectangle, then use that transformation to sample the image.</li> <li> <strong>Bonus (1 point)</strong>: Successfully handle the arbitrary four-corner coordinates. (If time is short, you may use a simpler rectangular crop, but it is strongly recommended to attempt the DLT approach.)</li> </ul> <h4 id="2-synthetic-perturbations-synthesis">2. Synthetic Perturbations (<code class="language-plaintext highlighter-rouge">synthesis</code>)</h4> <ul> <li>Write a function <strong><code class="language-plaintext highlighter-rouge">synthesis</code></strong> to generate training samples.</li> <li>Given an original region and a corresponding rectangle, apply <strong>small random transformations</strong> (e.g., translation, rotation, scaling, or even homography).</li> <li>Use a <strong>Gaussian distribution</strong> over the transformation parameters, as described in Travis et al.</li> <li>Start with smaller DOFs (e.g., translation only) and move on to more complex transformations (4 DOFs: rotation + scale, 6 DOFs: affine, 8 DOFs: full homography).</li> </ul> <h4 id="3-learning-the-tracker-learn">3. Learning the Tracker (<code class="language-plaintext highlighter-rouge">learn</code>)</h4> <ul> <li>Implement the core learning procedure for <strong>either</strong> the Hyperplane method (Jurie &amp; Dhome) <strong>or</strong> the Nearest Neighbor method (Travis et al.).</li> <li>This involves taking your large set (e.g., 1000–2000 samples) of synthetic transformations and the corresponding intensity differences, then learning how to predict the parameter updates from the intensity errors.</li> </ul> <h4 id="4-incremental-updating-update">4. Incremental Updating (<code class="language-plaintext highlighter-rouge">update</code>)</h4> <ul> <li>Implement the <strong>incremental update</strong> procedure, where at each tracking step you refine your estimate of the transformation parameters.</li> <li>Refer to the chosen paper (Jurie &amp; Dhome, or Travis et al.) to see how the learned model is updated (or how the nearest neighbor lookup is performed in each new frame).</li> </ul> <hr> <h3 id="what-to-submit">What to Submit:</h3> <ol> <li> <strong>Functions</strong>: <ul> <li> <code class="language-plaintext highlighter-rouge">sample_region</code>: warps a given region to a rectangle.</li> <li> <code class="language-plaintext highlighter-rouge">synthesis</code>: generates a training set of synthetic images under random transformations.</li> <li> <code class="language-plaintext highlighter-rouge">learn</code>: trains your model (Hyperplane or Nearest Neighbor).</li> <li> <code class="language-plaintext highlighter-rouge">update</code>: refines the transformation parameters incrementally.</li> </ul> </li> <li> <strong>Report Questions</strong>: <ul> <li>Describe the parameter settings you used for sampling (e.g., the range of translations, rotations, etc.).</li> <li>Compare the performance of your tracker on the sample videos or your own recorded videos.</li> <li>If you completed both methods (Hyperplane &amp; Nearest Neighbor), compare their speeds and accuracies.</li> </ul> </li> </ol> <hr> <h3 id="practical-tips">Practical Tips</h3> <ul> <li> <strong>Sample Generation</strong>: Generating 1000–2000 random samples often suffices. Start with a moderate region size (e.g., from <code class="language-plaintext highlighter-rouge">(50, 50)</code> to <code class="language-plaintext highlighter-rouge">(100, 100)</code> in your image).</li> <li> <strong>Performance</strong>: If you use Python + OpenCV, raw nearest neighbor lookups can be slow. Consider using libraries like <strong>pyflann</strong> or other approximate nearest neighbor libraries for faster queries.</li> <li> <strong>Debugging</strong>: Test on a <strong>“static image motion”</strong> experiment (e.g., synthetically warped single image) before moving to real video data.</li> </ul> <hr> <p><strong>Note</strong>: This exercise counts as <strong>two</strong> if you fully implement and demonstrate both the <strong>Hyperplane Approximation</strong> and the <strong>Nearest Neighbor Approximation</strong>. Make sure to manage your time and start with simpler transformations before attempting the full homography approach.</p> <hr> <h2 id="submission-details">Submission Details</h2> <ul> <li>Include accompanying code used to complete each question. Ensure they are adequately commented.</li> <li>Ensure all functions are and sections are clearly labeled in your report to match the tasks and deliverables outlined in the lab.</li> <li>Organize files as follows: <ul> <li> <code class="language-plaintext highlighter-rouge">code/</code> folder containing all scripts used in the assignment.</li> <li> <code class="language-plaintext highlighter-rouge">media/</code> folder for images, videos, and results.</li> </ul> </li> <li>Final submission format: a single zip file named <code class="language-plaintext highlighter-rouge">CompVisW25_lab1.2_lastname_firstname.zip</code> containing the above structure.</li> <li>Your combined report for Lab 1.1 and 1.2 is due shortly after (see calendar for details). The report contains all media, results, and answers as specified in the instructions above. Ensure your answers are concise and directly address the questions.</li> <li>Total marks for this lab is <strong>100</strong> for all students. Your lab assignment grade with bonus marks is capped at 110%. Report bonus marks will be applied to the report grade, also capped at 110%.</li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Computer Vision and Robotics Research Group. Created by Allie Luo and Justin Valentine. Adapted from <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/cmput428labs/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/cmput428labs/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/cmput428labs/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/cmput428labs/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/cmput428labs/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/cmput428labs/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/cmput428labs/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>