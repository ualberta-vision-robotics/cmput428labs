<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta http-equiv="Cache-Control" content="no-cache"> <meta http-equiv="Pragma" content="no-cache"> <meta http-equiv="Expires" content="Thu, 01 Jan 1970 00:00:00 GMT"> <title> Lab 1.2 - Tracking | CMPUT 428/615 Labs </title> <meta name="author" content="Computer Vision and Robotics Research Group "> <meta name="description" content="Lab resources for CMPUT428/615."> <meta name="keywords" content="cmput428, cmput615, ualberta, computing-science academic-website"> <link rel="stylesheet" href="/cmput428labs/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/cmput428labs/assets/img/favi.ico?824de67f88398f637ce988037dfe4447"> <link rel="stylesheet" href="/cmput428labs/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ualberta-vision-robotics.github.io/cmput428labs/tracking"> <script src="/cmput428labs/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/cmput428labs/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/cmput428labs/"> CMPUT 428/615 Labs </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/cmput428labs/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cmput428labs/policies">Policies </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Labs </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cmput428labs/opticalflow">Lab 1.1</a> <a class="dropdown-item " href="/cmput428labs/tracking">Lab 1.2</a> <a class="dropdown-item " href="/cmput428labs/2dgeometry">Lab 2.1</a> <a class="dropdown-item " href="/cmput428labs/3dgeometry">Lab 2.2</a> <a class="dropdown-item " href="/cmput428labs/structurefrommotion">Lab 3</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Lab 1.2 - Tracking</h1> <p class="post-description"></p> </header> <article> <div class="row justify-content-md-center"> <div class="col-sm-6 mt-6 mt-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cmput428labs/assets/img/tracker-480.webp 480w,/cmput428labs/assets/img/tracker-800.webp 800w,/cmput428labs/assets/img/tracker-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/cmput428labs/assets/img/tracker.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="optical flow" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Registration tracking </div> <h2 id="overview">Overview</h2> <p>In this lab you’ll implement the Lucas-Kanade tracking algorithm. The goal of this algorithm is to minimize the sum of squared error between two regions, the template \(T(\mathbf{x})\), and the warped image \(I(\mathbf{W}(\mathbf{x} ; \mathbf{p}))\):</p> <p>\begin{equation} \sum_{\mathbf{x}}[I(\mathbf{W}(\mathbf{x} ; \mathbf{p}))-T(\mathbf{x})]^2 \end{equation}</p> <p>\(\mathbf{p}\) is the vector of parameters that defines our warp. The most basic of which would be a simple 2-dof translation in Equation 2. Higher order warps such as an affine warp, given by Equation 3, or homography warps, given by Equation 4, can also be applied.</p> <p>\begin{equation} \mathrm{W}(\mathbf{x}, \mathbf{p})=\binom{\mathrm{x}+\mathrm{p}_1}{\mathrm{y}+\mathrm{p}_2} <br> \end{equation} \begin{equation} \mathrm{~W}(\mathbf{x}, \mathbf{p})=\binom{\mathrm{p}_1 \mathrm{x}+\mathrm{p}_3 \mathrm{y}+\mathrm{p}_5}{\mathrm{p}_2 \mathrm{x}+\mathrm{p}_4 \mathrm{y}+\mathrm{p}_6} <br> \end{equation} \begin{equation} \mathrm{~W}(\mathbf{x}, \mathbf{p})=\frac{1}{1+p_7 x+p_8 y}\binom{\mathrm{p}_1 \mathrm{x}+\mathrm{p}_3 \mathrm{y}+\mathrm{p}_5}{\mathrm{p}_2 \mathrm{x}+\mathrm{p}_4 \mathrm{y}+\mathrm{p}_6} \end{equation}</p> <p>The user first defines the tracking region of interest, which becomes the template. We aim to find the warped region in subsequent frames that best match our template. When a suitable match is found, we plot our tracking region and then repeat on the next frame.</p> <p>This <a href="https://drive.google.com/file/d/1kAU7-oK6aZh2zg5I8lLWZxnAShkkkClU/view?usp=drive_link" rel="external nofollow noopener" target="_blank">zip file</a> contains the sample videos you can use to complete this lab. However, you are encouraged to capture your own.</p> <p><strong>Important: You can obtain 4 bonus marks for each section you finish and demo while present during the lab period we introduce the lab.</strong></p> <hr> <h2 id="1-2-dof-ssd-tracker-60">1. 2-DoF SSD Tracker (60)</h2> <p>Create a function <code class="language-plaintext highlighter-rouge">simple_tracker(roi, im0, im1, max_iterations, threshold)</code> that takes in the roi and a pair of images and returns the coordinates of the updated tracking region.</p> <h3 id="a-apply-your-tracker-to-a-pair-of-images">a) Apply your tracker to a pair of images.</h3> <ul> <li>Outside of your function, display the first image and define a bounding box <code class="language-plaintext highlighter-rouge">roi</code> to track using <code class="language-plaintext highlighter-rouge">cv2.selectROI()</code>. <ul> <li>We obtain the template from <code class="language-plaintext highlighter-rouge">roi</code> and <code class="language-plaintext highlighter-rouge">im0</code>. This can be updated for each new frame or left to be the original template from the first frame.</li> </ul> </li> <li>For subsequent frames, use your function to solve for \(\mathbf{u}=[u,v]^T\) on the <code class="language-plaintext highlighter-rouge">roi</code> of the next frame like in optical flow. Your <code class="language-plaintext highlighter-rouge">roi</code> is the patch. <ul> <li>Use \(\mathbf{u}\) to update the tracked region using a 2 DoF transformation given by Equation 2. <ul> <li>Repeatedly solve for \(\mathbf{u}\) and update the tracked region until <code class="language-plaintext highlighter-rouge">norm(u_k)/norm(u_{k-1}) &lt; threshold</code> or <code class="language-plaintext highlighter-rouge">k &gt; max_iterations</code>, where <code class="language-plaintext highlighter-rouge">k</code> is the number of iterations (you can define your own end condition, this is just what we suggest).</li> </ul> </li> <li>Your function should return the new <code class="language-plaintext highlighter-rouge">roi</code>. Update and display the this bounding box overlaid onto <code class="language-plaintext highlighter-rouge">im1</code> (see <code class="language-plaintext highlighter-rouge">cv2.rectangle</code> for plotting).</li> <li>Note: We can choose to update the template every frame or we can keep the template we obtain from the first frame for all subsequent comparisons.</li> </ul> </li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Display the pair of images in your report.</li> </ul> <p><strong>Report Question 1a:</strong> Why do we have to iteratively warp our tracked region instead of just solve for \(\mathbf{u}\) once and update our bounding box?</p> <h3 id="b-create-a-live-implementation-of-your-tracker">b) Create a live implementation of your tracker.</h3> <ul> <li>Compare the results of using the same template on the first image for all subsequent frames vs updating the template for every new <code class="language-plaintext highlighter-rouge">roi</code> we find.</li> </ul> <p><strong>Report Question 1b:</strong> What is one benefit and drawback of updating the template every frame?</p> <h3 id="c-apply-your-tracker-to-a-video-sequence-that-you-recorded">c) Apply your tracker to a video sequence that you recorded.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save your video with the overlaid tracking region for your submission. Ensure the video is less than 10 mb.</li> </ul> <p><strong>Report Question 1c:</strong> In what cases does the tracker perform well? In what cases does the tracker perform poorly? Name one type of image processing that may improve the performance of your tracker.</p> <hr> <h2 id="important">Important</h2> <ul> <li> <p>Undergraduate students: choose <strong>one of the following three questions</strong> to complete for this lab to obtain the remaining 40 marks.</p> </li> <li> <p><strong><font color="DarkViolet">Grad students: choose two of the following three questions to obtain the remaining 60 marks.</font></strong></p> </li> </ul> <hr> <h2 id="2-pyramidal-2-dof-ssd-tracker">2. Pyramidal 2-DoF SSD Tracker</h2> <p>Create a function <code class="language-plaintext highlighter-rouge">pyramidal_tracker(roi, im0, im1, levels=4, scale=2)</code> that performs 2-DoF SSD tracking with pyramidal gaussian downsampling. You may call <code class="language-plaintext highlighter-rouge">simple_tracker()</code> within this function if you wish.</p> <ul> <li>The gaussian pyramid has <code class="language-plaintext highlighter-rouge">levels</code> layers, with each layer having a <code class="language-plaintext highlighter-rouge">scale</code> factor resolution of the layer above.</li> <li>See <code class="language-plaintext highlighter-rouge">cv2.pyrDown()</code> for gaussian downsampling.</li> <li>We obtain the template from <code class="language-plaintext highlighter-rouge">roi</code> and <code class="language-plaintext highlighter-rouge">im0</code>. This can be updated for each new frame or left to be the original template from the first frame.</li> <li>For each frame start with the top of the pyramid (coarsest) and solve for the new region. Propogate the result to the more detailed layer below and repeat for all layers. <ul> <li>Ensure the coordinates of the updated region match the scale factor of the layer below (i.e. multiply the translation by <code class="language-plaintext highlighter-rouge">scale</code>)</li> </ul> </li> <li>Update the tracked region with the bounding box of the bottom layer (the layer with no downsampling) and repeat for all frames.</li> </ul> <h3 id="apply-your-pyramidal-tracker-to-two-videos-that-you-record-one-with-camera-motion-and-one-with-object-motion">Apply your pyramidal tracker to two videos that you record, one with camera motion and one with object motion.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save the videos with overlaid tracking regions to include with your submission. Ensure they are less than 10 mb each.</li> </ul> <p><strong>Report Question 2a:</strong> Name one advantage and one disadvantage of implementing pyramidal downsampling to our tracker.</p> <p><strong>Report Question 2b:</strong> How do the number of pyramid levels and the chosen downsampling factor affect the trade-off between computational speed and accuracy?</p> <p><strong>Report Question 2c:</strong> Why do we use a coarse-to-fine strategy in pyramidal tracking, and are there any cases where a fine-to-coarse approach might be beneficial?</p> <hr> <h2 id="3-high-dof-tracker">3. High-DoF Tracker</h2> <p>Create a function <code class="language-plaintext highlighter-rouge">highdof_tracker(img0, img1, roi, max_iterations, threshold)</code> that warps the bounding box using 4 (x, y, rotation, scale) or warp the bounding box using 6 (affine) parameters \(\mathbf{p}\) for 10 additional bonus marks.</p> <ul> <li>See <a href="https://docs.opencv.org/4.x/da/d6e/tutorial_py_geometric_transformations.html" rel="external nofollow noopener" target="_blank">OpenCV documentation on transformations</a>.</li> <li>See <a href="https://www.ncorr.com/download/publications/bakerunify.pdf" rel="external nofollow noopener" target="_blank">Baker and Matthews Paper on Lukas-Kanade</a>.</li> <li>See <a href="https://ugweb.cs.ualberta.ca/~vis/courses/CompVis/lectures24/lec05bRegTrack2.pdf" rel="external nofollow noopener" target="_blank">page 27 of these slides</a>.</li> </ul> <h3 id="apply-your-tracker-to-two-videos-that-you-record-one-with-camera-motion-and-one-with-object-motion">Apply your tracker to two videos that you record, one with camera motion and one with object motion.</h3> <p><strong>Deliverables:</strong></p> <ul> <li>Save the videos with overlaid tracking regions to include with your submission. Ensure they are less than 10 mb each.</li> </ul> <p><strong>Report Question 3:</strong> Name one advantage and one disadvantage of using higher order warps for our tracker.</p> <hr> <h2 id="4-learning-based-tracking">4. Learning-Based Tracking</h2> <p>In traditional intensity-based tracking, we estimate the parameters of a transformation that warps a template (I_0) to match a region in the current image (I_t). Typically, this is approached by minimizing the intensity error between the template and the warped patch.</p> <p>However, instead of <strong>analytically computing</strong> the Jacobian (as you did in previous exercises), we now want to <strong>learn</strong> the mapping from image intensities to transformation parameters from a <strong>large set of synthetically generated training samples</strong>.</p> <p>You will implement <strong>one</strong> of the following methods (or <strong>both</strong>, for double credit):</p> <ol> <li> <p><strong>Hyperplane Approximation</strong><br> <em>Reference</em>: F. Jurie and M. Dhome, “Hyperplane approximation for template matching,” 2002.</p> </li> <li> <p><strong>Nearest Neighbor Approximation</strong> (Note: This is more difficult, you will receive <strong>10 bonus marks</strong> for completing this) <em>Reference</em>: D. Travis, C. Perez, A. Shademan, and M. Jagersand, “Realtime Registration-Based Tracking via Approximate Nearest Neighbour Search,” 2013.</p> </li> </ol> <p>A quick overview of these learning-based approaches can be found in (<code class="language-plaintext highlighter-rouge">lec05bRegTrack2.pdf</code> slides 47-52).</p> <h4 id="a-sampling-the-region-sample_region">a. Sampling the Region (<code class="language-plaintext highlighter-rouge">sample_region</code>)</h4> <ul> <li>Write a function <strong><code class="language-plaintext highlighter-rouge">sample_region</code></strong> that takes as input the four corner coordinates of a region in an image and returns a rectangular patch corresponding to that region.</li> <li>You can implement this by estimating a <strong>DLT (Direct Linear Transform)</strong> that warps the four-corner region into a fixed rectangle, then use that transformation to sample the image.</li> <li> <strong>Bonus (1)</strong>: Successfully handle the arbitrary four-corner coordinates. (If time is short, you may use a simpler rectangular crop, but it is strongly recommended to attempt the DLT approach.)</li> </ul> <p><strong>Report Question 4a</strong>: Describe the parameter settings you used for sampling (e.g., the range of translations, rotations, etc.).</p> <h4 id="b-synthetic-perturbations-synthesis">b. Synthetic Perturbations (<code class="language-plaintext highlighter-rouge">synthesis</code>)</h4> <ul> <li>Write a function <strong><code class="language-plaintext highlighter-rouge">synthesis</code></strong> to generate training samples.</li> <li>Given an original region and a corresponding rectangle, apply <strong>small random transformations</strong> (e.g., translation, rotation, scaling, or even homography).</li> <li>Use a <strong>Gaussian distribution</strong> over the transformation parameters, as described in Travis et al.</li> <li>Start with smaller DOFs (e.g., translation only) and move on to more complex transformations (4 DOFs: rotation + scale, 6 DOFs: affine, 8 DOFs: full homography).</li> </ul> <h4 id="c-learning-the-tracker-learn">c. Learning the Tracker (<code class="language-plaintext highlighter-rouge">learn</code>)</h4> <ul> <li>Implement the core learning procedure for <strong>either</strong> the Hyperplane method (Jurie &amp; Dhome) <strong>or</strong> the Nearest Neighbor method (Travis et al.).</li> <li>This involves taking your large set (e.g., 1000–2000 samples) of synthetic transformations and the corresponding intensity differences, then learning how to predict the parameter updates from the intensity errors.</li> </ul> <h4 id="d-incremental-updating-update">d. Incremental Updating (<code class="language-plaintext highlighter-rouge">update</code>)</h4> <ul> <li>Implement the <strong>incremental update</strong> procedure, where at each tracking step you refine your estimate of the transformation parameters.</li> <li>Refer to the chosen paper (Jurie &amp; Dhome, or Travis et al.) to see how the learned model is updated (or how the nearest neighbor lookup is performed in each new frame).</li> </ul> <p><strong>Report Question 4b</strong>: If you completed both methods (Hyperplane &amp; Nearest Neighbor), compare their speeds and accuracies.</p> <h3 id="practical-tips">Practical Tips</h3> <ul> <li> <strong>Sample Generation</strong>: Generating 1000–2000 random samples often suffices. Start with a moderate region size (e.g., from <code class="language-plaintext highlighter-rouge">(50, 50)</code> to <code class="language-plaintext highlighter-rouge">(100, 100)</code> in your image).</li> <li> <strong>Performance</strong>: If you use Python + OpenCV, raw nearest neighbor lookups can be slow. Consider using libraries like <strong>pyflann</strong> or other approximate nearest neighbor libraries for faster queries.</li> <li> <strong>Debugging</strong>: Test on a <strong>“static image motion”</strong> experiment (e.g., synthetically warped single image) before moving to real video data.</li> </ul> <p><strong>Note</strong>: This exercise counts as <strong>two</strong> if you fully implement and demonstrate both the <strong>Hyperplane Approximation</strong> and the <strong>Nearest Neighbor Approximation</strong>. Make sure to manage your time and start with simpler transformations before attempting the full homography approach.</p> <hr> <h2 id="submission-details">Submission Details</h2> <ul> <li>Include accompanying code used to complete each question. Ensure they are adequately commented.</li> <li>Ensure all functions are and sections are clearly labeled in your report to match the tasks and deliverables outlined in the lab.</li> <li>Organize files as follows: <ul> <li> <code class="language-plaintext highlighter-rouge">code/</code> folder containing all scripts used in the assignment.</li> <li> <code class="language-plaintext highlighter-rouge">media/</code> folder for images, videos, and results.</li> </ul> </li> <li>Final submission format: a single zip file named <code class="language-plaintext highlighter-rouge">CompVisW25_lab1.2_lastname_firstname.zip</code> containing the above structure.</li> <li>Your combined report for Lab 1.1 and 1.2 is due shortly after (see calendar for details). The report contains all media, results, and answers as specified in the instructions above. Ensure your answers are concise and directly address the questions.</li> <li>Total marks for this lab is <strong>100</strong> for undergraduate students and <strong>120</strong> for graduate students. Your lab assignment grade with bonus marks is capped at <strong>120%</strong>. Report bonus marks will be applied to the report grade, also capped at 110%.</li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Computer Vision and Robotics Research Group. Created by Allie Luo and Justin Valentine. Adapted from <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/cmput428labs/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/cmput428labs/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/cmput428labs/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/cmput428labs/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/cmput428labs/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/cmput428labs/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/cmput428labs/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>