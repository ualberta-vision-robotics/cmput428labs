<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta http-equiv="Cache-Control" content="no-cache"> <meta http-equiv="Pragma" content="no-cache"> <meta http-equiv="Expires" content="Thu, 01 Jan 1970 00:00:00 GMT"> <title> Lab 2.2 - 3D Projective Geometry and Stereo Reconstruction | CMPUT 428/615 Labs </title> <meta name="author" content="Computer Vision and Robotics Research Group "> <meta name="description" content="Lab resources for CMPUT428/615."> <meta name="keywords" content="cmput428, cmput615, ualberta, computing-science academic-website"> <link rel="stylesheet" href="/cmput428labs/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/cmput428labs/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/cmput428labs/assets/img/favi.ico?824de67f88398f637ce988037dfe4447"> <link rel="stylesheet" href="/cmput428labs/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ualberta-vision-robotics.github.io/cmput428labs/3dgeometry"> <script src="/cmput428labs/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/cmput428labs/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/cmput428labs/"> CMPUT 428/615 Labs </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/cmput428labs/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cmput428labs/policies">Policies </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Labs </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cmput428labs/opticalflow">Lab 1.1</a> <a class="dropdown-item " href="/cmput428labs/tracking">Lab 1.2</a> <a class="dropdown-item " href="/cmput428labs/2dgeometry">Lab 2.1</a> <a class="dropdown-item " href="/cmput428labs/3dgeometry">Lab 2.2</a> <a class="dropdown-item " href="/cmput428labs/structurefrommotion">Lab 3</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Lab 2.2 - 3D Projective Geometry and Stereo Reconstruction</h1> <p class="post-description"></p> </header> <article> <div class="row justify-content-md-center"> <div class="col-sm-8 mt-6 mt-md-6"> <figure> <video src="/cmput428labs/assets/img/cubes1.webm" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="volume up to hear the cubes march to a great song" autoplay="" loop="" muted=""></video> </figure> </div> </div> <div class="caption"> Using perspective projections and 3D transformation matrices to create walking cubes. Source material from [<a href="https://darla.com/products/sweet-trip-velocity-design-comfort" target="_blank" rel="external nofollow noopener">1</a>]. </div> <h2 id="overview">Overview</h2> <p>In this lab you’ll learn about 3D projective geometry and stereo reconstruction.</p> <p><strong>Important: You can obtain 2 bonus marks for each section you finish and demo while present during the lab period we introduce the lab.</strong></p> <hr> <h2 id="1-homogeneous-transforms-and-projections-30">1. Homogeneous Transforms and Projections (30)</h2> <p>A transformatrion matrix, \(M\), can be used to transform 3D points in homogeneous form:</p> <p>\begin{equation} \left(\begin{array}{l} \displaylines{x’\\ y’\\ z’\\w’} \end{array}\right) = M \left(\begin{array}{l} \displaylines{x\\ y\\ z\\1} \end{array}\right) \end{equation}</p> <p>We define translation and rotation matricies in homogenenous coordinates as follows:</p> <p>\begin{equation} T=\left[\begin{array}{cccc} \displaylines{ 1 &amp; 0 &amp; 0 &amp; t_x\\ 0 &amp; 1 &amp; 0 &amp; t_y\\ 0 &amp; 0 &amp; 1 &amp; t_z\\ 0 &amp; 0 &amp; 0 &amp; 1 } \end{array}\right] \end{equation}</p> <p>\begin{equation} R = R_zR_yR_x \end{equation}</p> <p>\begin{equation} R_z=\left[\begin{array}{cccc} \displaylines{ cos\theta &amp; -sin\theta &amp; 0 &amp; 0\\ sin\theta &amp; cos\theta &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 } \end{array}\right] \end{equation}</p> <p>\begin{equation} R_y=\left[\begin{array}{cccc} \displaylines{ cos\theta &amp; 0 &amp; sin\theta &amp; 0\\ 0 &amp; 1 &amp; 0 &amp; 0\\ -sin\theta &amp; 0 &amp; cos\theta &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 } \end{array}\right] \end{equation}</p> <p>\begin{equation} R_x=\left[\begin{array}{cccc} \displaylines{ 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; cos\theta &amp; -sin\theta &amp; 0\\ 0 &amp; sin\theta &amp; cos\theta &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 } \end{array}\right] \end{equation}</p> <p>\begin{equation} S=\left[\begin{array}{cccc} \displaylines{ s_x &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; s_y &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; s_z &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 } \end{array}\right] \end{equation}</p> <h3 id="a-generating-3d-point-clouds">a) Generating 3D Point Clouds</h3> <div class="row justify-content-md-center"> <div class="col-sm-10 mt-6 mt-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/cmput428labs/assets/img/point_cloud-480.webp 480w,/cmput428labs/assets/img/point_cloud-800.webp 800w,/cmput428labs/assets/img/point_cloud-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/cmput428labs/assets/img/point_cloud.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="point clouds" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Uniformly sampled points used to create point clouds. </div> <p>The following code block creates a line with randomly sampled points from \(X\sim U(0,1)\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">numpoints</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">numpoints</span><span class="p">)</span>
<span class="n">zeros</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">numpoints</span><span class="p">))</span>
<span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">numpoints</span><span class="p">))</span>

<span class="n">x_l</span> <span class="o">=</span> <span class="n">t</span>
<span class="n">y_l</span> <span class="o">=</span> <span class="n">zeros</span>
<span class="n">z_l</span> <span class="o">=</span> <span class="n">zeros</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">row_stack</span><span class="p">((</span><span class="n">x_l</span><span class="p">,</span> <span class="n">y_l</span><span class="p">,</span> <span class="n">z_l</span><span class="p">,</span> <span class="n">ones</span><span class="p">))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="sh">'</span><span class="s">3d</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_box_aspect</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x_l</span><span class="p">,</span> <span class="n">y_l</span><span class="p">,</span> <span class="n">z_l</span><span class="p">,</span> <span class="sh">'</span><span class="s">gray</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">x line point cloud</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <ul> <li>Recreate the plots above featuring a line, a circle, and a cube.</li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Display your figure in your report.</li> <li>Save your figure to include in your submission.</li> </ul> <p><strong>Report Question 1a:</strong> What type of projection is being used to render these point clouds on our screen?</p> <h3 id="b-3d-homogeneous-transformations">b) 3D Homogeneous Transformations</h3> <ul> <li>Create functions that return the 3D transformation matrices defined above. Use them to apply the following transformations on your cube point cloud: <ol> <li>A rotation about at least two axes.</li> <li>A translation and a scaling.</li> <li>Create an equally-spaced <strong>facade of 4 cubes</strong>. Apply one rotation and scaling operation of your choice to all 4 cubes.</li> </ol> </li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Display figures of your transformed cubes in your report.</li> <li>Save figures of your transformed cubes to include with your submission.</li> </ul> <p><strong>Report Question 1b:</strong> In what situations do rotations also translate the cube?</p> <h3 id="c-projections">c) Projections</h3> <ol> <li>Apply orthographic projection to the <strong>first two</strong> transformed cube point clouds you created in b)</li> <li>Apply perspective projection to the <strong>first two</strong> transformed cube point clouds you created in b)</li> </ol> <p><strong>Deliverables:</strong></p> <ul> <li>Display your four projections in your report.</li> <li>Save your four projections to include with your submission.</li> </ul> <p><strong>Report Question 1c:</strong> Name two differences between orthographic projection and perspective projection.</p> <h3 id="d-bonus-10---animation">d) Bonus (10) - Animation</h3> <ul> <li>Create an animation using transformations and projections you’ve learned in this course thus far. First, create the animation in 3D and then project it to 2D.</li> <li>If you wish to place your animation into a scene, you must recover the camera matrix given a set of 2D and 3D point correspondences by modifying your DLT algorithm (<a href="https://ugweb.cs.ualberta.ca/~vis/courses/CompVis/lectures18/lec06ProjG2D.pdf" rel="external nofollow noopener" target="_blank">see page 54-62 of these slides</a>) <ul> <li>This is not mandatory, but it is the mechanism that allows for your projections to “match” a scene. The cover video for this page was created in this manner.</li> </ul> </li> <li>Be creative! An additional 5 bonus marks will be awarded cool-factor, as judged by the TAs. Minimal marks will be awarded for trivial animations.</li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Save the animation to include with your submission</li> </ul> <p><strong>Report Question 1d:</strong> Briefly describe how you created your animation.</p> <hr> <h2 id="2-estimating-focal-length-10">2. Estimating Focal Length (10)</h2> <ul> <li>Determine the focal length of your phone camera by using it to take a picture of a ruler a known depth away.</li> <li>Make sure the ruler is placed perpendicular to the camera axis (i.e. the ruler is square to the camera).</li> <li>Use similar triangles formed with the ruler and its projection in the image to find your camera’s focal length in pixels (<a href="https://ugweb.cs.ualberta.ca/~vis/courses/CompVis/lectures18/lec06GeomIntro.pdf" rel="external nofollow noopener" target="_blank">See page 68-72 of these slides</a>.</li> <li>Convert the focal length to mm using the ratio of our photo’s width in pixels to your phone’s camera sensor width in mm.</li> <li>Convert the focal length in mm to its full-frame equivalent by using the diagonal sensor width ratios between your phone camera and a 35 mm film camera, also known as the crop factor (<a href="https://en.wikipedia.org/wiki/Image_sensor_format" rel="external nofollow noopener" target="_blank">reference the crop factor of your camera sensor in this table</a>)</li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Write the focal length in pixels and mm (full-frame equivalent) and the manufacturer’s specified focal length as a comment in your code.</li> <li>Write the focal length in pixels and mm (full-frame equivalent) and the manufacturer’s specified focal length in your report.</li> </ul> <p><strong>Report Question 2</strong> What is the focal length of your phone camera? How does this compare to the manufacturers’ specifications?</p> <hr> <h2 id="3-ssd-based-stereo-depth-estimation-20">3 SSD-based Stereo Depth Estimation (20)</h2> <p>Implement a window-based Sum of Squared Differences (SSD) method for stereo matching to compute a disparity map. Then, convert it to a depth map using known camera parameters.</p> <h3 id="a-disparity-map-computation-ssd">a) Disparity Map Computation (SSD):</h3> <ul> <li>You are provided the calibrated Tsukuba stereo pair (<a href="/cmput428labs/assets/img/tsukuba_l.png" target="_blank">left</a> and <a href="/cmput428labs/assets/img/tsukuba_r.png" target="_blank">right</a> images) with the following camera parameters: <ul> <li> <strong>Focal Length:</strong> \(f = 300\) pixels</li> <li> <strong>Baseline:</strong> \(B = 0.16\) meters</li> </ul> </li> <li>Write a program that implements the SSD method for computing disparity: <ul> <li>For each pixel in the left image, extract a square window of size \((2w+1) \times (2w+1)\) (e.g. \(w=3\) for a 7×7 window).</li> <li>For each candidate disparity \(d\) in a specified range (e.g. from \(-d_{\text{max}}\) to \(d_{\text{max}}\)), compute the matching cost using the formula:</li> </ul> </li> </ul> <p style="text-align:center;"> $$ \text{SSD}(x, y, d) = \sum_{i=-w}^{w} \sum_{j=-w}^{w} \left[ I_L(x+i, \, y+j) - I_R(x+i-d, \, y+j) \right]^2 $$ </p> <ul> <li>Select the disparity \(d^*(x,y)\) that minimizes the SSD for each pixel.</li> </ul> <p><strong>Report Question 3a:</strong></p> <ul> <li>How do the choices of window size and disparity search range affect the accuracy and noise level of your disparity and depth maps? What are the potential limitations of using the SSD approach, especially in textureless or occluded regions?</li> <li>In the tracking lab you had the option of implementing a pyramidal tracker; could a similar pyramidal strategy be used here to increase performance? How would it work?</li> </ul> <p><strong>Deliverables:</strong></p> <ul> <li>Save the the disparity map to include with your submission.</li> <li>Place the disparity map in your report.</li> </ul> <h3 id="b-depth-map-computation">b) Depth Map Computation:</h3> <ul> <li>Convert the computed disparity map \(d^*(x,y)\) to a depth map using the stereo geometry relationship:</li> </ul> <p style="text-align:center;"> $$ Z(x,y) = \frac{f \cdot B}{|d^*(x,y)|} $$ </p> <ul> <li> <em>Hint:</em> Refer to <a href="https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf" rel="external nofollow noopener" target="_blank">pages 12-14 of these slides</a> for guidance on the depth computation process.</li> <li> <p>Be sure to handle cases where the disparity is zero or nearly zero (e.g., by thresholding or marking these pixels as invalid).</p> </li> <li> <strong>Bonus (5)</strong> - Modify your program to apply a simple smoothing or median filter to the disparity map before converting it to depth. In your report, explain how this post-processing step improves (or degrades) the quality of the final 3D reconstruction.</li> </ul> <p><strong>Report Question 3b:</strong> Would our reconstruction still work if the translation is not perpendicular to the camera axis?</p> <p><strong>Deliverables:</strong></p> <ul> <li>Save the the depth map to include with your submission.</li> <li>Place the depth map in your report.</li> </ul> <hr> <h2 id="4-2-image-stereo-reconstruction-15">4. 2-Image Stereo Reconstruction (15)</h2> <p>In this question you will use point correspondence to perform 3D reconstruction. You will work with your own camera and calculated camera parameters to determine the depth of the feature points. Reconstruct the 3D coordinates in a camera-centered coordinate system, and texture your 3D model.</p> <ol> <li> <strong>Image Acquisition:</strong> <ul> <li> <strong>Setup:</strong> Place a camera perpendicular to a ruler pointed at a box object.</li> <li> <strong>Capture:</strong> Take an image of the object.</li> <li> <strong>Shift and Capture Again:</strong> Move the camera laterally by a known distance (measured by the ruler) and take a second picture.</li> </ul> </li> <li> <strong>Feature Correspondence:</strong> <ul> <li>Select the corner points (or other feature points) in both images to create a set of corresponding feature points.</li> </ul> </li> <li> <strong>Depth Computation:</strong> <ul> <li>Determine the depth of the feature points. This should be the same as you did in question 3.<br> <em>Hint:</em> Refer to <a href="https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect16.pdf" rel="external nofollow noopener" target="_blank">pages 12-14 of these slides</a> for guidance on the depth computation process.</li> </ul> </li> <li> <strong>3D Reconstruction:</strong> <ul> <li>Use the computed depths to reconstruct the 3D coordinates of the feature points in a camera-centered coordinate system.</li> <li>Plot the reconstructed points in a 3D plot.</li> </ul> </li> <li> <strong>Texture:</strong> <ul> <li>Texture your 3D object. For example, texture rectangular faces by calling <code class="language-plaintext highlighter-rouge">plot_surface()</code> for each face (See <a href="/cmput428labs/plot_surface" target="_blank">plot_surface() example</a>).</li> </ul> </li> </ol> <p><strong>Report Question 4:</strong> Would our reconstruction still work if the translation is not perpendicular to the camera axis?</p> <p><strong>Deliverables:</strong></p> <ul> <li>Save and include the 3D plot of the reconstructed object.</li> <li>Save and include the two images you captured.</li> <li>Incorporate these results in your final report.</li> </ul> <hr> <h2 id="5-n-image-stereo-reconstruction-25">5. n-Image Stereo Reconstruction (25)</h2> <p>In this section you will extend your 2-image stereo reconstruction pipeline to handle multiple images (\(n\) images). You will work either synthetic data or real data captured by you.</p> <h3 id="a-n-image-stereo-reconstruction">a) n-Image Stereo Reconstruction</h3> <ol> <li> <strong>3D Object Creation:</strong> <ul> <li>Build a 3D representation of a box object.</li> </ul> </li> <li> <strong>Projection onto Multiple Images:</strong> <ul> <li>Project the 3D structure onto a set of \(n = 10\) images.</li> <li> <strong>Camera Setup:</strong> Assume the images are taken along a line perpendicular to the camera axis at fixed intervals.</li> <li> <strong>Perspective Projection:</strong> Use a camera with the focal length determined in Question 2 to perform the projection.</li> </ul> </li> <li> <strong>Depth Determination with Multiple Views:</strong> <ul> <li>Formulate a least squares system to determine the depth of the feature points using all \(n\) images.</li> <li>Reconstruct the 3D coordinates and compare the results with your original 3D structure.</li> </ul> </li> <li> <strong>Texture:</strong> <ul> <li>Texture your 3D object by applying <code class="language-plaintext highlighter-rouge">plot_surface()</code> to texture each rectangular face (See <a href="/cmput428labs/plot_surface" target="_blank">plot_surface() example</a>).</li> </ul> </li> </ol> <p><strong>Deliverables:</strong></p> <ul> <li>Save and include the 3D plot of the reconstructed object.</li> <li>Save and include one example projection image.</li> <li>Add these results to your report.</li> </ul> <p><strong>Report Question 5a:</strong> How does the reconstruction process differ as we increase the number of images in our dataset?</p> <h4 id="b-bonus-15---real-data">b) Bonus (15) - Real Data</h4> <ol> <li> <strong>Video Acquisition:</strong> <ul> <li>Record a video of a moving object (or a moving camera) along a line perpendicular to the camera axis.<br> <em>Note:</em> Sliding your object/camera against a fixed surface can improve recording quality.</li> </ul> </li> <li> <strong>Feature Tracking:</strong> <ul> <li>Use a tracker to follow a set of feature points that characterize the object’s structure throughout the video (e.g., the corners of a box).</li> </ul> </li> <li> <strong>Depth Computation with Multiple Views:</strong> <ul> <li>Formulate a least squares system to determine the depth of the feature points using all frames (treated as individual images) from the video.</li> <li>Reconstruct the 3D coordinates and compare the results with the starting structure.</li> </ul> </li> <li> <strong>Texture:</strong> <ul> <li>Texture your 3D object by applying <code class="language-plaintext highlighter-rouge">plot_surface()</code> for each face (See <a href="/cmput428labs/plot_surface" target="_blank">plot_surface() example</a>).</li> </ul> </li> </ol> <p><strong>Deliverables:</strong></p> <ul> <li>Save and include the 3D plot of the reconstructed object.</li> <li>Save and include the recording (video) used for the reconstruction.</li> <li>Include one representative frame from the video with the features you tracked marked in your final report.</li> </ul> <hr> <hr> <h2 id="submission-details">Submission Details</h2> <ul> <li>Include accompanying code used to complete each question. Ensure they are adequately commented.</li> <li>Ensure all functions are and sections are clearly labeled in your report to match the tasks and deliverables outlined in the lab.</li> <li>Organize files as follows: <ul> <li> <code class="language-plaintext highlighter-rouge">code/</code> folder containing all scripts used in the assignment.</li> <li> <code class="language-plaintext highlighter-rouge">media/</code> folder for images, videos, and results.</li> </ul> </li> <li>Final submission format: a single zip file named <code class="language-plaintext highlighter-rouge">CompVisW25_lab2.2_lastname_firstname.zip</code> containing the above structure.</li> <li>Your combined report for Lab 2.1 and 2.2 is due shortly after (see calendar for details). The report contains all media, results, and answers as specified in the instructions above. Ensure your answers are concise and directly address the questions.</li> <li>Total marks for this lab is <strong>100</strong> for all students. Your lab assignment grade with bonus marks is capped at <strong>130%</strong>.</li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Computer Vision and Robotics Research Group. Created by Allie Luo and Justin Valentine. Adapted from <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/cmput428labs/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/cmput428labs/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/cmput428labs/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/cmput428labs/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/cmput428labs/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/cmput428labs/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/cmput428labs/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/cmput428labs/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>