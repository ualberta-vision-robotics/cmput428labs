---
layout: page
title: Lab 1.2 - Tracking
description:
permalink: /tracking
img: 
importance: 2
category: Motion Estimation and Tracking
related_publications: false
---

<div class="row justify-content-md-center">
    <div class="col-sm-3 mt-4 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/tracker.gif" title="optical flow" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    To change: filler image of tracking a banana.
</div>

## Overview

In this lab you'll implement the Lucas-Kanade tracking algorithm. The goal of this algorithm is to minimize the sum of squared error between two regions, the template $$T(\mathbf{x})$$, and the warped image $$I(\mathbf{W}(\mathbf{x} ; \mathbf{p}))$$:

\begin{equation}
\sum_{\mathbf{x}}[I(\mathbf{W}(\mathbf{x} ; \mathbf{p}))-T(\mathbf{x})]^2
\end{equation}

$$\mathbf{p}$$ is the vector of parameters that defines our warp. The most basic of which would be a simple 2-dof translation. Higher order warps such as an affine warp can also be applied.

The user first defines the tracking region of interest, which becomes the first template. We aim to find the warped region in the next frame that best matches our template. When a suitable match is found, we set the warped region as the new template and repeat on the next frame.

<!-- This [zip file]({{"/assets/labs/sample_videos.zip" | relative_url}}){:target="_blank"} contains the sample videos you can use to complete this lab. However, you are encouraged to capture your own. -->

__Important: You can obtain 4 bonus marks for each section you finish and demo while present during the lab period we introduce the lab.__

---

## 1. 2-Dof SSD Tracker (50)

Create a function `simple_tracker(img0, img1, roi, max_iterations, threshold)` takes a pair of sequenced images and a tracking region and returns the coordinates of the updated tracking region.

### a) Apply your tracker to a pair of images.
- Display the first image and define a bounding box to track using `cv2.selectROI()`. This defines the `roi` for your function.
- For the next frame, solve for $$\mathbf{u}=[u,v]^T$$ on the tracked region like in optical flow. Your tracked region is the patch.
  - Use $$\mathbf{u}$$ to update the tracked region.
    - Repeatedly solve for $$\mathbf{u}$$ and update the tracking region until `norm($$u_k$$)/norm($$u_{k-1}$$) < threshold` or `k < max_iterations`, where `k` is the number of iterations (you can define your own end condition, this is just what we suggest).
  - Your function should return the new `roi`. Update and display the this bounding box as the new tracked region (see `cv2.rectangle` for plotting).

**Deliverables:**
- Display the pair of images in your report.

**Report Question 1a:** Name one type of image processing that may improve the performance of your tracker.

### b) Create a live implementation of your tracker.

**Report Question 1b:** In what cases does the tracker perform well? In what cases does the tracker perform poorly?

### c) Apply your tracker to a video sequence that you recorded.

**Deliverables:**
- Save your video with the overlaid tracking region for your submission. Ensure the video is less than 10 mb.

**Report Question 1c:** Name one advantage and one disadvantage of using higher order warps for our tracker.

---
###__Important__
- Undergraduate students: choose __one of the following three questions__ to complete for this lab to obtain the remaining 50 marks.

- <strong><font color='DarkViolet'>Grad students: complete question 2 and 3 OR complete question 4 to obtain the remaining 50 marks.</font></strong>

---

## 2. Pyramidal 2-Dof SSD Tracker
Create a function `pyramidal_tracker(img0, img1, roi, levels=4, scale=2)` that performs 2-Dof SSD tracking with pyramidal guassian downsampling. You may call `simple_tracker()` within this function if you wish.

- The gaussian pyramid has `levels` layers, with each layer having a `scale` factor resolution of the layer above.
- See `cv2.pyrDown()` for gaussian downsampling.
- For each frame start with the top of the pyramid (coarsest) and solve for the new region. Propogate the result to the more detailed layer below and repeat for all layers.
  - Ensure the coordinates of the updated region match the scale factor of the layer below (i.e. multiply the translation by `scale`)
- Update the tracked region with the bounding box of the bottom layer (the layer with no downsampling) and repeat for all frames.

### Apply your pyramidal tracker to a video that you recorded.

**Deliverables:**
- Save this video to include with your submission. Ensure it is less than 10 mb.

**Report Question 2:** What are two benefits of implementing pyramidal downsampling to our tracker.

---

## 3. High-Dof Tracker
Create a function `highdof_tracker(img0, img1, roi, max_iterations, threshold)` that warps the bounding box using 4 (x, y, rotation, scale) or 6 (affine) parameters $$\mathbf{p}$$.

- See [OpenCV documentation on transformations](https://docs.opencv.org/4.x/da/d6e/tutorial_py_geometric_transformations.html).
- See [Baker and Matthews Paper on Lukas-Kanade](https://www.ncorr.com/download/publications/bakerunify.pdf)
- See [page 27 of these slides](https://ugweb.cs.ualberta.ca/~vis/courses/CompVis/lectures24/lec05bRegTrack2.pdf)

### a) Apply your tracker to a video with object motion.

**Deliverables:**
- Save the video to include in your submission. Ensure it is less than 10 mb.

**Report Question 1a:** lorem ipsum?


### b) Apply your tracker to a video with camera motion.

**Deliverables:**
- Save the video to include in your submission. Ensure it is less than 10 mb.

**Report Question 1a:** lorem ipsum?

---

## 4. Learning-Based Tracking

In traditional intensity-based tracking, we estimate the parameters of a transformation that warps a template \(I_0\) to match a region in the current image \(I_t\). Typically, this is approached by minimizing the intensity error between the template and the warped patch.

However, instead of **analytically computing** the Jacobian (as you did in previous exercises), we now want to **learn** the mapping from image intensities to transformation parameters from a **large set of synthetically generated training samples**.

You will implement **one** of the following methods (or **both**, for double credit):

1. **Hyperplane Approximation**  
   *Reference*: F. Jurie and M. Dhome, “Hyperplane approximation for template matching,” 2002.

2. **Nearest Neighbor Approximation**  
   *Reference*: D. Travis, C. Perez, A. Shademan, and M. Jagersand, “Realtime Registration-Based Tracking via Approximate Nearest Neighbour Search,” 2013.

A quick overview of these learning-based approaches can be found in (`lec05bRegTrack2.pdf` slides 47-52).
#### a. Sampling the Region (`sample_region`)
- Write a function **`sample_region`** that takes as input the four corner coordinates of a region in an image and returns a rectangular patch corresponding to that region.  
- You can implement this by estimating a **DLT (Direct Linear Transform)** that warps the four-corner region into a fixed rectangle, then use that transformation to sample the image.  
- **Bonus (1 point)**: Successfully handle the arbitrary four-corner coordinates. (If time is short, you may use a simpler rectangular crop, but it is strongly recommended to attempt the DLT approach.)

#### b. Synthetic Perturbations (`synthesis`)
- Write a function **`synthesis`** to generate training samples.  
- Given an original region and a corresponding rectangle, apply **small random transformations** (e.g., translation, rotation, scaling, or even homography).  
- Use a **Gaussian distribution** over the transformation parameters, as described in Travis et al.
- Start with smaller DOFs (e.g., translation only) and move on to more complex transformations (4 DOFs: rotation + scale, 6 DOFs: affine, 8 DOFs: full homography).

#### c. Learning the Tracker (`learn`)
- Implement the core learning procedure for **either** the Hyperplane method (Jurie & Dhome) **or** the Nearest Neighbor method (Travis et al.).  
- This involves taking your large set (e.g., 1000–2000 samples) of synthetic transformations and the corresponding intensity differences, then learning how to predict the parameter updates from the intensity errors.

#### d. Incremental Updating (`update`) (Bonus)
- Implement the **incremental update** procedure, where at each tracking step you refine your estimate of the transformation parameters.  
- Refer to the chosen paper (Jurie & Dhome, or Travis et al.) to see how the learned model is updated (or how the nearest neighbor lookup is performed in each new frame).

 **Report Question 4a**:  
   - Describe the parameter settings you used for sampling (e.g., the range of translations, rotations, etc.).  
   
 **Report Question 4b**: 
   - If you completed both methods (Hyperplane & Nearest Neighbor), compare their speeds and accuracies.

---
### Practical Tips
- **Sample Generation**: Generating 1000–2000 random samples often suffices. Start with a moderate region size (e.g., from `(50, 50)` to `(100, 100)` in your image).  
- **Performance**: If you use Python + OpenCV, raw nearest neighbor lookups can be slow. Consider using libraries like **pyflann** or other approximate nearest neighbor libraries for faster queries.  
- **Debugging**: Test on a **“static image motion”** experiment (e.g., synthetically warped single image) before moving to real video data.

---

**Note**: This exercise counts as **two** if you fully implement and demonstrate both the **Hyperplane Approximation** and the **Nearest Neighbor Approximation**. Make sure to manage your time and start with simpler transformations before attempting the full homography approach.

---

## Submission Details

- Include accompanying code used to complete each question. Ensure they are adequately commented.
- Ensure all functions are and sections are clearly labeled in your report to match the tasks and deliverables outlined in the lab.
- Organize files as follows:
  - `code/` folder containing all scripts used in the assignment.
  - `media/` folder for images, videos, and results.
- Final submission format: a single zip file named `CompVisW25_lab1.2_lastname_firstname.zip` containing the above structure.
- Your combined report for Lab 1.1 and 1.2 is due shortly after (see calendar for details). The report contains all media, results, and answers as specified in the instructions above. Ensure your answers are concise and directly address the questions.
- Total marks for this lab is __100__ for all students. Your lab assignment grade with bonus marks is capped at 110%. Report bonus marks will be applied to the report grade, also capped at 110%.


